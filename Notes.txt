Phase 1: Data Modeling                              -- DONE
Phase 2: API Development                            -- DONE
Phase 3: Logging                                    -- DONE
Phase 4: Testing                                    -- 
Phase 5: Caching                                    -- DONE
Phase 6: Scaling & Deployment                       -- DONE
Phase 7: Pagination                                 -- DONE
Phase 8: Validation                                 -- DONE
Phase 9: Error Handling                             -- DONE
Phase 10: Docker                                    -- DONE
Phase 11: Kubernetes                                -- DONE
Phase 12: CI CD                                     -- DONE
Phase 13 Open API using sawgger                     -- DONE
Phase 14 Health check points                        -- DONE
Phase 15 Monitoring                                 -- 
Phase 16 Rate limiting                              -- DONE
Phase 17 Observability                              -- 

---------------------------------------------------------------------------
Request â†’ [Logger] â†’ [Zod Validation] â†’ Controller â†’ Service â†’ Database (If anything fails) â†˜ [Global Error Handler] â†’ Response
---------------------------------------------------------------------------
Tools for Measurement
â€¢ Profiling: Measures where an application spends its time during execution (e.g., CPU-bound tasks)
â€¢ Flame Graphs: A visualization tool for profiling data, showing the call stack and time spent in functions
â€¢ Profilers are excellent for CPU-bound tasks but struggle with IO-bound tasks (database queries, external API calls)
â€¢ Distributed Tracing: Tracks a request as it flows through the entire system, recording time spent at every step (DB, service layer, external calls)
---------------------------------------------------------------------------
5 layers: The Big Picture

TypeScript code
   â†“
Prisma Client (generated code)
   â†“
Prisma Adapter (pg / Supabase pooler)
   â†“
Postgres (Supabase)
   â†“
Docker (packaging + runtime)
---------------------------------------------------------------------------

Prisma lifecycle

schema.prisma
    â†“
prisma generate   â† build time
    â†“
TypeScript compile
    â†“
Runtime Prisma Client
---------------------------------------------------------------------------
Prisma generates a typed client from the schema.
That client must exist before TypeScript compiles.
Docker builds the app without envs, so Prisma needs a dummy DB URL at build time.
Real credentials are injected only when the container runs.
---------------------------------------------------------------------------
Prisma:
  schema â†’ generate â†’ types â†’ runtime queries

TypeScript:
  source â†’ compile â†’ dist
---------------------------------------------------------------------------
Monitoring answers three questions:

Is the service alive? â†’ health check 
Is it slow or failing? â†’ request metrics
Why did it fail? â†’ structured logs

---------------------------------------------------------------------------
You now have:

âœ… Health checks (availability)
âœ… Request metrics (performance)
âœ… Error tracking (reliability)
âœ… Structured logs (debugging)

This is observability, not logging.

What NOT to do yet

âŒ Prometheus
âŒ Grafana
âŒ OpenTelemetry
âŒ Tracing systems

Those come after infra exists.
---------------------------------------------------------------------------
Availability
/health endpoint
Docker health check

Performance

Request latency
Slow endpoints
Traffic patterns

Reliability

Error frequency
Error types
Failing routes
---------------------------------------------------------------------------
What problems CI will now catch early

âœ” Missing DATABASE_URL
âœ” Prisma client mismatch
âœ” Broken Dockerfile
âœ” TypeScript errors
âœ” Missing files in image
---------------------------------------------------------------------------
What Kubernetes does now

If probe fails:

readiness â†’ pod removed from traffic
liveness â†’ pod restarted

You now have self-healing.
---------------------------------------------------------------------------
âœ” Dockerized backend
âœ” CI validation
âœ” Image publishing
âœ” Kubernetes Deployment
âœ” Secrets
âœ” Health probes
âœ” Service
âœ” Self-healing system
âœ… Multi-replica backend
âœ… Resource protection
âœ… Health-based routing
âœ… Zero-downtime rolling updates
âœ… Instant rollback capability
Ingress routing
---------------------------------------------------------------------------
A rolling update:

â€œHow do I replace running pods with new ones without stopping traffic?â€

Kubernetes does not:

stop all old pods
then start new ones

Instead, it:

brings up new pods
waits until theyâ€™re ready
shifts traffic
then removes old pods

This only works because:

you have readiness probes
your app is stateless
traffic goes through a Service
---------------------------------------------------------------------------
How a real rolling update happens

When you deploy a new image version:
Kubernetes creates 1 new pod
Waits for readiness probe to pass
Routes traffic to it
Deletes 1 old pod
Repeats until done
---------------------------------------------------------------------------
In production, traffic looks like:

Internet
   â†“
Load Balancer / Reverse Proxy
   â†“
Ingress Controller
   â†“
Kubernetes Service
   â†“
Pods

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml
kubectl get pods -n ingress-nginx
kubectl get svc -n ingress-nginx

NAME                                 TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             LoadBalancer   10.96.74.233   <pending>     80:32419/TCP,443:30816/TCP   86s
ingress-nginx-controller-admission   ClusterIP      10.96.248.83   <none>        443/TCP                      86s

kubectl get ingress -n todo-backend
kubectl logs -n ingress-nginx deploy/ingress-nginx-controller

â€œIngress is the entry point to the cluster.
It routes external traffic to services based on host and path.
Services abstract pod lifecycles, so rolling updates donâ€™t cause downtime.â€

Ingress is just: A smart reverse proxy running inside the cluster

In your case:

That proxy is NGINX
It runs as a Pod
It listens on ports 80 / 443
It reads Ingress rules
It forwards traffic to Services

Traffic flow in Development
Browser
  â†“
/etc/hosts (todo.local â†’ 127.0.0.1)
  â†“
NodePort (32419)
  â†“
Ingress NGINX Pod
  â†“
Ingress rules (Host + Path)
  â†“
Service
  â†“
Pod

Traffic flow in production
Internet
  â†“
DNS (api.pavii.dev â†’ LB IP)
  â†“
Cloud LoadBalancer
  â†“
Ingress Controller
  â†“
Service
  â†“
Pods
---------------------------------------------------------------------------
When someone asks:

â€œHow does traffic reach your service in Kubernetes?â€

Your answer:

â€œDNS resolves the domain to the load balancer.
The load balancer forwards traffic to an ingress controller.
Ingress routes based on host/path to a service, which load-balances across pods.â€
---------------------------------------------------------------------------

---------------------------------------------------------------------------

---------------------------------------------------------------------------

ğŸ”¹ Option A â€” HTTPS (TLS with cert-manager)

Real certificates, HTTPS, production realism

ğŸ”¹ Option B â€” Autoscaling (HPA)

Traffic-based scaling (very impressive)

ğŸ”¹ Option C â€” Secrets & ConfigMaps

Production-safe config management

ğŸ‘‰ Tell me which one you want next