Phase 1: Data Modeling                              -- DONE
Phase 2: API Development                            -- DONE
Phase 3: Logging                                    -- DONE
Phase 4: Testing                                    -- DONE
Phase 5: Caching                                    -- DONE
Phase 6: Scaling & Deployment                       -- DONE
Phase 7: Pagination                                 -- DONE
Phase 8: Validation                                 -- DONE
Phase 9: Error Handling                             -- DONE
Phase 10: Docker                                    -- DONE
Phase 11: Kubernetes                                -- DONE
Phase 12: CI CD                                     -- DONE
Phase 13 Open API using sawgger                     -- DONE
Phase 14 Health check points                        -- DONE
Phase 15 Monitoring                                 -- DONE
Phase 16 Rate limiting                              -- DONE
Phase 17 Observability                              -- DONE
Phase 18 Authentication                             -- DONE
Phase 19 Queue                                      -- 
Phase 20 Security                                   -- DONE
Phase 21 Code Quality                               -- DONE
Phase 22 Code Rabit                                 -- DONE
Phase 23 qodo-code-review                           -- DONE
Phase 24 Linting and Formatig                       -- DONE
Phase 25 Open Telemetry                             -- DONE
Phase 26 Grafana and Prometus                       -- DONE

  
---------------------------------------------------------------------------
Request → [Logger] → [Zod Validation] → Controller → Service → Database (If anything fails) ↘ [Global Error Handler] → Response
---------------------------------------------------------------------------
5 layers: The Big Picture

TypeScript code
   ↓
Prisma Client (generated code)
   ↓
Prisma Adapter (pg / Supabase pooler)
   ↓
Postgres (Supabase)
   ↓
Docker (packaging + runtime)
---------------------------------------------------------------------------

Prisma lifecycle

schema.prisma
    ↓
prisma generate   ← build time
    ↓
TypeScript compile
    ↓
Runtime Prisma Client
---------------------------------------------------------------------------
Prisma generates a typed client from the schema.
That client must exist before TypeScript compiles.
Docker builds the app without envs, so Prisma needs a dummy DB URL at build time.
Real credentials are injected only when the container runs.
---------------------------------------------------------------------------
Prisma:
  schema → generate → types → runtime queries

TypeScript:
  source → compile → dist
---------------------------------------------------------------------------
Monitoring answers three questions:

Is the service alive? → health check 
Is it slow or failing? → request metrics
Why did it fail? → structured logs

---------------------------------------------------------------------------
You now have:

✅ Health checks (availability)
✅ Request metrics (performance)
✅ Error tracking (reliability)
✅ Structured logs (debugging)

This is observability, not logging.

What NOT to do yet

❌ Prometheus
❌ Grafana
❌ OpenTelemetry
❌ Tracing systems

Those come after infra exists.
---------------------------------------------------------------------------
Availability
/health endpoint
Docker health check

Performance

Request latency
Slow endpoints
Traffic patterns

Reliability

Error frequency
Error types
Failing routes
---------------------------------------------------------------------------
What problems CI will now catch early

✔ Missing DATABASE_URL
✔ Prisma client mismatch
✔ Broken Dockerfile
✔ TypeScript errors
✔ Missing files in image
---------------------------------------------------------------------------
What Kubernetes does now

If probe fails:

readiness → pod removed from traffic
liveness → pod restarted

You now have self-healing.
---------------------------------------------------------------------------
✔ Dockerized backend
✔ CI validation
✔ Image publishing
✔ Kubernetes Deployment
✔ Secrets
✔ Health probes
✔ Service
✔ Self-healing system
✅ Multi-replica backend
✅ Resource protection
✅ Health-based routing
✅ Zero-downtime rolling updates
✅ Instant rollback capability
Ingress routing
---------------------------------------------------------------------------
A rolling update:

“How do I replace running pods with new ones without stopping traffic?”

Kubernetes does not:

stop all old pods
then start new ones

Instead, it:

brings up new pods
waits until they’re ready
shifts traffic
then removes old pods

This only works because:

you have readiness probes
your app is stateless
traffic goes through a Service
---------------------------------------------------------------------------
How a real rolling update happens

When you deploy a new image version:
Kubernetes creates 1 new pod
Waits for readiness probe to pass
Routes traffic to it
Deletes 1 old pod
Repeats until done
---------------------------------------------------------------------------
In production, traffic looks like:

Internet
   ↓
Load Balancer / Reverse Proxy
   ↓
Ingress Controller
   ↓
Kubernetes Service
   ↓
Pods

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml
kubectl get pods -n ingress-nginx
kubectl get svc -n ingress-nginx

NAME                                 TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             LoadBalancer   10.96.74.233   <pending>     80:32419/TCP,443:30816/TCP   86s
ingress-nginx-controller-admission   ClusterIP      10.96.248.83   <none>        443/TCP                      86s

kubectl get ingress -n todo-backend
kubectl logs -n ingress-nginx deploy/ingress-nginx-controller

“Ingress is the entry point to the cluster.
It routes external traffic to services based on host and path.
Services abstract pod lifecycles, so rolling updates don’t cause downtime.”

Ingress is just: A smart reverse proxy running inside the cluster

In your case:

That proxy is NGINX
It runs as a Pod
It listens on ports 80 / 443
It reads Ingress rules
It forwards traffic to Services

kubectl port-forward -n ingress-nginx svc/ingress-nginx-controller 8080:80
This means:

“Create a tunnel from my laptop port 8080 → ingress controller port 80 inside Kubernetes”

No Docker networking.
No NodePort.
No LoadBalancer.
Just a direct pipe.

Traffic flow in Development
curl
 ↓
todo.local → 127.0.0.1
 ↓
localhost:8080   (port-forward)
 ↓
Ingress Controller (NGINX)
 ↓
Ingress rule: Host = todo.local
 ↓
Service: todo-backend
 ↓
Healthy Pod
 ↓
Express app
 ↓
/health/database


Traffic flow in production
Internet
  ↓
DNS (api.pavii.dev → LB IP)
  ↓
Cloud LoadBalancer
  ↓
Ingress Controller
  ↓
Service
  ↓
Pods

“Ingress defines routing inside Kubernetes, but it doesn’t expose traffic by itself. In production, a cloud load balancer forwards traffic to the ingress controller. 
In local environments like kind, we use port-forwarding or node port mappings to simulate that exposure.
---------------------------------------------------------------------------
When someone asks:

“How does traffic reach your service in Kubernetes?”

Your answer:

“DNS resolves the domain to the load balancer.
The load balancer forwards traffic to an ingress controller.
Ingress routes based on host/path to a service, which load-balances across pods.”
---------------------------------------------------------------------------
Headers
Key: Host
Value: todo.local

http://localhost:8080/api/v1/auth/login

{
    "email": "pavi@gmail.com",
    "password": "pavi@123"
}

 "success": true,
    "message": "Login successful",
    "token": "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6IjMwYTAyNzBhLWRjNzUtNDVhMC1iNWUwLTkyMGFhYTgzNzc2MiIsImlhdCI6MTc2ODAzMjk1OCwiZXhwIjoxNzY4MDM2NTU4fQ.uPvRo5pXEajJtPfoNCv5Pvh75p1-BOVYQcX46yZP2og",
    "user": {
        "email": "pavi@gmail.com"
    }


In production:

Port-forward = ❌ never
Cloud Load Balancer = ✅ always


Local (kind)	  Production
port-forward	  AWS ALB / GCP LB
localhost:8080	  api.todo.com
/etc/hosts	     Route53 / Cloud DNS
manual Host      header/real DNS
---------------------------------------------------------------------------
HPA 

HPA = Horizontal Pod Autoscaler

It answers one question repeatedly:

“Do I need more pods or fewer pods right now?”

It does this by watching metrics (CPU / memory / custom metrics).

kubectl get apiservices | grep metrics
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

kubectl top pods -n todo-backend

NAME                            CPU(cores)   MEMORY(bytes)   
todo-backend-5dfb449b57-954jk   1m           36Mi
todo-backend-5dfb449b57-svvdr   0m           36Mi
todo-backend-5dfb449b57-wmssf   1m           34Mi

kubectl get deploy todo-backend -n todo-backend -o yaml | grep -A10 resources

resources:
          limits:
            cpu: 500m
            memory: 256Mi
          requests:
            cpu: 100m
            memory: 128Mi

kubectl apply -f hpa.yaml
kubectl get hpa -n todo-backend

NAME               REFERENCE                 TARGETS              MINPODS   MAXPODS   REPLICAS   AGE
todo-backend-hpa   Deployment/todo-backend   cpu: 0%/50%             2         4         3          7s

kubectl describe hpa todo-backend-hpa -n todo-backendnd

Name:                                                  todo-backend-hpa
Namespace:                                             todo-backend
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Sat, 10 Jan 2026 08:25:45      
Reference:                                             Deployment/todo-backend
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  0% (0) / 50%

---------------------------------------------------------------------------
HTTPS with cert-manager

kubectl apply -f https://github.com/cert-manager/cert-manager/releases/latest/download/cert-manager.yaml
kubectl get pods -n cert-manager

NAME                                       READY   STATUS    RESTARTS   AGE
cert-manager-859f5d5d75-8mz9z              1/1     Running   0          36s
cert-manager-cainjector-6569c5c766-p7mdm   1/1     Running   0          36s
cert-manager-webhook-54896b88d9-gnpg9      1/1     Running   0          36s

kubectl apply -f cluster-issuer.yaml
kubectl get clusterissuer

NAME               READY   AGE
letsencrypt-prod   True    6s

kubectl apply -f ingress.yaml

kubectl describe certificate -n todo-backend

---------------------------------------------------------------------------------
pavii@PraveenUppar:/mnt/c/Users/pavii/Desktop/Todo$ kubectl cluster-info
Kubernetes control plane is running at https://127.0.0.1:46417
CoreDNS is running at https://127.0.0.1:46417/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy


pavii@PraveenUppar:/mnt/c/Users/pavii/Desktop/Todo$ kubectl get nodes
NAME                         STATUS   ROLES           AGE     VERSION
todo-backend-control-plane   Ready    control-plane   5m35s   v1.30.0


pavii@PraveenUppar:/mnt/c/Users/pavii/Desktop/Todo$ kubectl get pods -A
NAMESPACE            NAME                                                 READY   STATUS    RESTARTS   AGE
kube-system          coredns-7db6d8ff4d-kzbxr                             1/1     Running   0          5m29s
kube-system          coredns-7db6d8ff4d-rsfd2                             1/1     Running   0          5m29s
kube-system          etcd-todo-backend-control-plane                      1/1     Running   0          5m44s
kube-system          kindnet-bdb4t                                        1/1     Running   0          5m30s
kube-system          kube-apiserver-todo-backend-control-plane            1/1     Running   0          5m44s
kube-system          kube-controller-manager-todo-backend-control-plane   1/1     Running   0          5m44s
kube-system          kube-proxy-5z8x6                                     1/1     Running   0          5m30s
kube-system          kube-scheduler-todo-backend-control-plane            1/1     Running   0          5m44s
local-path-storage   local-path-provisioner-988d74bc-sp4bf                1/1     Running   0          5m29s

pavii@PraveenUppar:/mnt/c/Users/pavii/Desktop/Todo$ kubectl get namespaces
NAME                 STATUS   AGE
default              Active   7m32s
kube-node-lease      Active   7m32s
kube-public          Active   7m32s
kube-system          Active   7m32s
local-path-storage   Active   7m26s
todo-backend         Active   13s

pavii@PraveenUppar:/mnt/c/Users/pavii/Desktop/Todo$ kubectl get deployments -n todo-backend
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
todo-backend   1/1     1            1           2m18s

pavii@PraveenUppar:/mnt/c/Users/pavii/Desktop/Todo$ kubectl get pods -n todo-backend
NAME                           READY   STATUS    RESTARTS   AGE
todo-backend-65cdc495b-p5skx   1/1     Running   0          2m21s

pavii@PraveenUppar:/mnt/c/Users/pavii/Desktop/Todo$ kubectl get secrets -n todo-backend
NAME                   TYPE     DATA   AGE
todo-backend-secrets   Opaque   6      9s

pavii@PraveenUppar:/mnt/c/Users/pavii/Desktop/Todo$ kubectl port-forward -n todo-backend service/todo-backend 3000:80
Forwarding from 127.0.0.1:3000 -> 3000
Forwarding from [::1]:3000 -> 3000
Handling connection for 3000
Handling connection for 3000

pavii@PraveenUppar:/mnt/c/Users/pavii/Desktop/Todokubectl rollout status deployment todo-backend -n todo-backendnd
Waiting for deployment "todo-backend" rollout to finish: 2 out of 3 new replicas have been updated...

kubectl rollout undo deployment todo-backend -n todo-backend

kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml

pavii@PraveenUppar:/mnt/c/Users/pavii/Desktop/Todokubectl get pods -n ingress-nginxnx
NAME                                        READY   STATUS    RESTARTS   AGE
ingress-nginx-controller-6775c6fd56-g6jfp   1/1     Running   0          82s

pavii@PraveenUppar:/mnt/c/Users/pavii/Desktop/Todo$ kubectl get svc -n ingress-nginx
NAME                                 TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE
ingress-nginx-controller             LoadBalancer   10.96.74.233   <pending>     80:32419/TCP,443:30816/TCP   86s
ingress-nginx-controller-admission   ClusterIP      10.96.248.83   <none>        443/TCP                      86s

 kubectl port-forward -n ingress-nginx svc/ingress-nginx-controller 8080:80
Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80
Handling connection for 8080